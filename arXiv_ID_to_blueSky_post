import pandas as pd
import re
import json
from collections import defaultdict

file_path = "paper_posts_initial.csv"  
df = pd.read_csv(file_path)

# Updated regex pattern to capture different arXiv ID formats, including URLs
arxiv_regex_updated = re.compile(
    r'(?:arxiv[:\s]*|https?://(?:www\.)?arxiv\.org/(?:abs|pdf)/)(\d{4}\.\d{4,5}(?:v\d+)?)',
    re.IGNORECASE
)

# Dictionary to map arXiv ID -> [Bluesky posts]
arxiv_mapping_updated = defaultdict(list)

# Iterate over rows and extract arXiv IDs using the updated regex
for _, row in df.iterrows():
    search_text = str(row.get('SearchText', ''))  # Ensure it's a string
    arxiv_ids = arxiv_regex_updated.findall(search_text)  # Extract arXiv IDs

    for arxiv_id in arxiv_ids:
        # Store relevant post details
        arxiv_mapping_updated[arxiv_id].append({
            "at_uri": row["at_uri"],
            "AuthorDID": row["AuthorDID"],
            "CreatedDate": row["CreatedDate"],
            "CID": row["CID"],
            "ReplyParent": row["ReplyParent"],
            "ReplyRoot": row["ReplyRoot"],
            "SearchText": search_text
        })

# Convert the mapping into a JSON structure
arxiv_json_mapping = json.dumps(arxiv_mapping_updated, indent=4)

# Print or return the JSON string
print(arxiv_json_mapping)


