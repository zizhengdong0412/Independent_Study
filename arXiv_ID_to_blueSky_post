import pandas as pd
import re
import json
from collections import defaultdict

file_path = "paper_posts_initial.csv"  
df = pd.read_csv(file_path)

# Updated regex pattern to capture different arXiv ID formats, including URLs
arxiv_regex_updated = re.compile(
    r'(?:arxiv[:\s]*|https?://(?:www\.)?arxiv\.org/(?:abs|pdf)/)(\d{4}\.\d{4,5}(?:v\d+)?)',
    re.IGNORECASE
)

# Dictionary to map arXiv ID -> [Bluesky posts]
arxiv_mapping_v2 = defaultdict(list)

# Iterate over rows and extract arXiv IDs using the updated regex
for _, row in df.iterrows():
    search_text = str(row.get('SearchText', ''))  # Ensure it's a string
    arxiv_ids = arxiv_regex_updated.findall(search_text)  # Extract arXiv IDs

    for arxiv_id in arxiv_ids:
        # Store relevant post details
        arxiv_mapping_v2[arxiv_id].append({
            "at_uri": row["at_uri"],
            "AuthorDID": row["AuthorDID"],
            "CreatedDate": row["CreatedDate"],
            "CID": row["CID"],
            "ReplyParent": row["ReplyParent"],
            "ReplyRoot": row["ReplyRoot"],
            "SearchText": search_text
        })

arxiv_df_updated = pd.DataFrame(
    [(arxiv_id, post["at_uri"], post["AuthorDID"], post["CreatedDate"], post["SearchText"])
     for arxiv_id, posts in arxiv_mapping_v2.items() for post in posts],
    columns=["arXiv_ID", "at_uri", "AuthorDID", "CreatedDate", "SearchText"]
)

with open("arxiv_bluesky_mapping.json", "w", encoding="utf-8") as f:
    json.dump(arxiv_mapping_v2, f, indent=4)

print("JSON mapping saved as 'arxiv_bluesky_mapping.json'")

# If csv file needed
csv_filename = "arxiv_bluesky_mapping.csv"
arxiv_df_updated.to_csv(csv_filename, index=False)
print(f"\n Extracted arXiv mapping saved as '{csv_filename}'")


